{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**"
      ],
      "metadata": {
        "id": "WoUKy4eLmrwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is a Decision Tree?**\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning algorithm used for **classification** and **regression** tasks. In the context of **classification**, it is used to predict the **class label** of an instance by learning decision rules inferred from the features of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Does a Decision Tree Work in Classification?**\n",
        "\n",
        "A **classification decision tree** breaks down a dataset into smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with **decision nodes** and **leaf nodes**.\n",
        "\n",
        "Here's a step-by-step breakdown:\n",
        "\n",
        "1. **Root Node Creation**:\n",
        "\n",
        "   * The process starts at the **root node**, which represents the entire dataset.\n",
        "   * A feature is selected that best splits the data into classes (using criteria like **Gini Impurity**, **Entropy/Information Gain**, or **Chi-square**).\n",
        "\n",
        "2. **Splitting**:\n",
        "\n",
        "   * The dataset is split into subsets based on the selected feature.\n",
        "   * This process is recursively repeated on each subset (creating branches and nodes) until one of the stopping conditions is met (e.g., all samples belong to one class or a maximum depth is reached).\n",
        "\n",
        "3. **Leaf Nodes**:\n",
        "\n",
        "   * The endpoints of the tree are called **leaf nodes**, and each leaf represents a class label (the majority class of the samples that reach that leaf).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose we want to classify whether someone will buy a product based on features like:\n",
        "\n",
        "* Age: Young, Middle-aged, Old\n",
        "* Income: Low, Medium, High\n",
        "* Student: Yes, No\n",
        "\n",
        "A decision tree might look like this:\n",
        "\n",
        "```\n",
        "         [Student?]\n",
        "         /       \\\n",
        "      Yes         No\n",
        "     /             \\\n",
        "[Buy=Yes]       [Income?]\n",
        "                  /   \\\n",
        "              High   Low\n",
        "              /         \\\n",
        "        [Buy=No]   [Buy=Yes]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Decision Trees in Classification**:\n",
        "\n",
        "* Easy to understand and interpret.\n",
        "* Can handle both numerical and categorical data.\n",
        "* No need for feature scaling.\n",
        "* Can model non-linear relationships.\n",
        "\n",
        "### **Disadvantages**:\n",
        "\n",
        "* Prone to **overfitting**, especially with deep trees.\n",
        "* Small changes in data can lead to a completely different tree.\n",
        "* Can be biased toward features with more levels (in categorical data).\n"
      ],
      "metadata": {
        "id": "Km-cXRqdmvOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "TsObyY-am9C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2: Gini Impurity and Entropy in Decision Trees**\n",
        "\n",
        "In a **decision tree**, the algorithm needs to decide **which feature** to split on at each step. To make this decision, it uses **impurity measures** like **Gini Impurity** and **Entropy** to evaluate how \"pure\" (i.e., homogeneous) a node is. The goal is to choose the feature that results in the **most significant reduction in impurity**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **1. Gini Impurity**\n",
        "\n",
        "### ‚û§ **Definition**:\n",
        "\n",
        "Gini Impurity measures the **probability** of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the node.\n",
        "\n",
        "### ‚û§ **Formula**:\n",
        "\n",
        "For a node with ( C ) classes:\n",
        "\n",
        "[\n",
        "\\text{Gini}(t) = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( p_i ) is the proportion of samples belonging to class ( i ) in node ( t ).\n",
        "\n",
        "### ‚û§ **Interpretation**:\n",
        "\n",
        "* A Gini Impurity of **0** means the node is **pure** (only one class present).\n",
        "* Higher values mean more mixed classes (less pure).\n",
        "* Maximum Gini for binary classification is **0.5** (when classes are perfectly split 50/50).\n",
        "\n",
        "---\n",
        "\n",
        "## üî• **2. Entropy (Information Gain)**\n",
        "\n",
        "### ‚û§ **Definition**:\n",
        "\n",
        "Entropy measures the **amount of uncertainty** or **disorder** in a node. It comes from **information theory** and quantifies the information needed to classify a sample.\n",
        "\n",
        "### ‚û§ **Formula**:\n",
        "\n",
        "[\n",
        "\\text{Entropy}(t) = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( p_i ) is the proportion of samples of class ( i ).\n",
        "\n",
        "### ‚û§ **Interpretation**:\n",
        "\n",
        "* Entropy = **0** when the node is pure (no disorder).\n",
        "* Entropy = **1** for a binary classification with equal distribution (maximum disorder).\n",
        "* Used to calculate **Information Gain**:\n",
        "  [\n",
        "  \\text{Information Gain} = \\text{Entropy(parent)} - \\sum \\left( \\frac{n_{\\text{child}}}{n_{\\text{parent}}} \\times \\text{Entropy(child)} \\right)\n",
        "  ]\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è **Gini vs. Entropy ‚Äì Key Differences**\n",
        "\n",
        "| Criteria              | Gini Impurity                    | Entropy (Information Gain)           |\n",
        "| --------------------- | -------------------------------- | ------------------------------------ |\n",
        "| Calculation           | Faster (no log operations)       | Slower (uses logarithms)             |\n",
        "| Interpretation        | Probability of misclassification | Information needed to classify       |\n",
        "| Output Range (Binary) | 0 to 0.5                         | 0 to 1                               |\n",
        "| Splitting Preference  | Tends to create **purer nodes**  | More sensitive to class distribution |\n",
        "\n",
        "In practice, **both often yield similar trees**, and many libraries (like scikit-learn) default to **Gini** for performance reasons.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Impact on Splits in a Decision Tree**\n",
        "\n",
        "At each node, the algorithm:\n",
        "\n",
        "1. Computes the impurity (Gini or Entropy) for all possible splits.\n",
        "2. Selects the split that leads to the **greatest reduction in impurity** (i.e., highest Information Gain or Gini Gain).\n",
        "3. Repeats this process recursively to build the tree.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example:\n",
        "\n",
        "Suppose a node contains:\n",
        "\n",
        "* 10 samples: 4 of Class A, 6 of Class B.\n",
        "\n",
        "* **Gini**:\n",
        "  [\n",
        "  G = 1 - (0.4)^2 - (0.6)^2 = 1 - 0.16 - 0.36 = 0.48\n",
        "  ]\n",
        "\n",
        "* **Entropy**:\n",
        "  [\n",
        "  H = -0.4 \\log_2(0.4) - 0.6 \\log_2(0.6) \\approx 0.971\n",
        "  ]\n",
        "\n",
        "The algorithm will compute such values for all potential splits and choose the one that reduces impurity the most.\n"
      ],
      "metadata": {
        "id": "V0ho3fZnnB0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "j7dfCiXDnTWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3: Pre-Pruning vs Post-Pruning in Decision Trees**\n",
        "\n",
        "In decision trees, **pruning** is a technique used to reduce the size of a tree and prevent **overfitting**, which happens when a model learns noise in the training data instead of general patterns.\n",
        "\n",
        "There are two main types of pruning:\n",
        "\n",
        "---\n",
        "\n",
        "## üå± **1. Pre-Pruning (Early Stopping)**\n",
        "\n",
        "### ‚û§ **Definition**:\n",
        "\n",
        "Pre-pruning stops the tree **from growing too large** by setting constraints **before or during** the tree-building process.\n",
        "\n",
        "### ‚û§ **How it works**:\n",
        "\n",
        "You define conditions to stop further splitting of a node, such as:\n",
        "\n",
        "* Maximum depth of the tree (`max_depth`)\n",
        "* Minimum number of samples to split a node (`min_samples_split`)\n",
        "* Minimum gain in impurity required to split (`min_impurity_decrease`)\n",
        "* Maximum number of leaves (`max_leaf_nodes`)\n",
        "\n",
        "### ‚úÖ **Practical Advantage**:\n",
        "\n",
        "> **Faster training time** ‚Äì because the tree doesn't grow unnecessarily deep, making it efficient for large datasets or real-time systems.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÇÔ∏è **2. Post-Pruning (Reduced Error Pruning)**\n",
        "\n",
        "### ‚û§ **Definition**:\n",
        "\n",
        "Post-pruning allows the tree to grow fully, and then **removes unnecessary branches** after the full tree is built, based on validation performance.\n",
        "\n",
        "### ‚û§ **How it works**:\n",
        "\n",
        "* Build the full tree (which may overfit).\n",
        "* Use a **validation set** or **cross-validation** to evaluate the performance of subtrees.\n",
        "* Recursively **remove branches** that do not improve (or worsen) predictive performance.\n",
        "\n",
        "### ‚úÖ **Practical Advantage**:\n",
        "\n",
        "> **Better generalization** ‚Äì by evaluating subtrees against actual performance, it often results in simpler, more accurate models on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Summary Table\n",
        "\n",
        "| Feature          | Pre-Pruning                      | Post-Pruning                              |\n",
        "| ---------------- | -------------------------------- | ----------------------------------------- |\n",
        "| **When applied** | During tree construction         | After full tree is built                  |\n",
        "| **Goal**         | Prevent overgrowth               | Remove overfitting branches               |\n",
        "| **Control**      | By setting limits (e.g., depth)  | By evaluating subtrees on validation data |\n",
        "| **Advantage**    | Faster training, less complexity | Better accuracy, improved generalization  |\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Real-World Example:\n",
        "\n",
        "* **Pre-Pruning**: In real-time fraud detection, you might want a shallow tree for fast predictions ‚Äî use `max_depth=3`.\n",
        "* **Post-Pruning**: In a medical diagnosis model, you build a full tree, then prune unnecessary splits based on validation to avoid overfitting and increase reliability.\n"
      ],
      "metadata": {
        "id": "5Xgb5MShnZku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?**"
      ],
      "metadata": {
        "id": "cZfaHT-wnkan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4: What is Information Gain in Decision Trees, and Why Is It Important for Choosing the Best Split?**\n",
        "\n",
        "---\n",
        "\n",
        "## üìò **What is Information Gain?**\n",
        "\n",
        "**Information Gain (IG)** is a metric used in decision trees (especially those using **Entropy**) to measure how much **uncertainty (or disorder)** in the data is **reduced** after a dataset is split based on a feature.\n",
        "\n",
        "> It tells us **how much information** a feature gives us about the class label.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ **Mathematically**:\n",
        "\n",
        "[\n",
        "\\text{Information Gain} = \\text{Entropy (Parent Node)} - \\sum_{i=1}^{k} \\frac{n_i}{n} \\cdot \\text{Entropy (Child Node}_i)\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( n ): Total samples in the parent node.\n",
        "* ( n_i ): Number of samples in child node ( i ).\n",
        "* ( k ): Number of child nodes after the split (usually 2).\n",
        "* **Entropy** is the impurity measure used to quantify the disorder of a node:\n",
        "  [\n",
        "  \\text{Entropy} = -\\sum p_i \\log_2(p_i)\n",
        "  ]\n",
        "\n",
        "---\n",
        "\n",
        "## üí° **Why Is Information Gain Important for Splitting?**\n",
        "\n",
        "At each step of building a decision tree, the algorithm must decide:\n",
        "\n",
        "> \"Which feature should I split on to best separate the data?\"\n",
        "\n",
        "Information Gain helps answer this by:\n",
        "\n",
        "* Quantifying **how useful** a feature is for reducing class impurity.\n",
        "* Preferring splits that result in **pure subsets** (i.e., each subset mostly contains one class).\n",
        "* Ensuring the tree grows in a direction that **best classifies** the training data.\n",
        "\n",
        "The **feature with the highest Information Gain** is chosen for the split.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Example**:\n",
        "\n",
        "Imagine we have a dataset with 10 samples:\n",
        "\n",
        "* 5 are **Yes** (positive class)\n",
        "* 5 are **No** (negative class)\n",
        "\n",
        "Initial entropy = 1 (maximum uncertainty).\n",
        "\n",
        "We try to split based on a feature \"Is Student?\" and get:\n",
        "\n",
        "* Group 1: 4 Yes, 1 No ‚Üí Entropy = 0.72\n",
        "* Group 2: 1 Yes, 4 No ‚Üí Entropy = 0.72\n",
        "\n",
        "Weighted entropy after split = 0.72\n",
        "\n",
        "**Information Gain** = 1.00 (original) ‚àí 0.72 = **0.28**\n",
        "\n",
        "If another feature gives higher IG (say 0.40), the algorithm will choose **that** feature instead.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Key Takeaways**:\n",
        "\n",
        "* Information Gain **measures the effectiveness** of an attribute in classifying the data.\n",
        "* Higher Information Gain ‚Üí Better feature for splitting.\n",
        "* It helps the decision tree grow in a way that **minimizes impurity**, leading to better performance.\n"
      ],
      "metadata": {
        "id": "gnYaJrXOnoi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**"
      ],
      "metadata": {
        "id": "U4v8A-U6n5xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5: Real-World Applications of Decision Trees, and Their Advantages & Limitations**\n",
        "\n",
        "---\n",
        "\n",
        "## üåç **Real-World Applications of Decision Trees**\n",
        "\n",
        "Decision trees are widely used in many fields due to their simplicity and interpretability. Here are some common applications:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Healthcare**\n",
        "\n",
        "* **Use**: Diagnosing diseases based on symptoms, lab results, and patient history.\n",
        "* **Example**: A tree that classifies whether a patient has diabetes based on glucose level, BMI, and age.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Finance**\n",
        "\n",
        "* **Use**: Credit scoring, fraud detection, risk assessment.\n",
        "* **Example**: Determining whether a loan applicant is likely to default based on income, credit history, and debt.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Marketing**\n",
        "\n",
        "* **Use**: Customer segmentation, predicting churn, targeting campaigns.\n",
        "* **Example**: Predicting whether a customer will respond to a promotional email based on purchase history and demographics.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. Retail & E-commerce**\n",
        "\n",
        "* **Use**: Recommending products, managing inventory.\n",
        "* **Example**: Predicting which products are likely to be out of stock soon based on sales trends and seasons.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. Manufacturing**\n",
        "\n",
        "* **Use**: Quality control, predictive maintenance.\n",
        "* **Example**: Predicting machine failure based on temperature, vibration, and usage time.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **6. Education**\n",
        "\n",
        "* **Use**: Student performance prediction, dropout risk analysis.\n",
        "* **Example**: Identifying at-risk students based on attendance, grades, and engagement levels.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è **Advantages of Decision Trees**\n",
        "\n",
        "| Advantage                                         | Explanation                                                        |\n",
        "| ------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| ‚úÖ **Easy to understand**                          | Clear, visual structure; no complex math needed to interpret.      |\n",
        "| ‚úÖ **Handles both numerical and categorical data** | No need for one-hot encoding or normalization.                     |\n",
        "| ‚úÖ **Requires little data preprocessing**          | Can handle missing values and outliers reasonably well.            |\n",
        "| ‚úÖ **Non-linear relationships**                    | Can model complex decision boundaries.                             |\n",
        "| ‚úÖ **Fast prediction**                             | Once trained, decision trees are very quick at making predictions. |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è **Limitations of Decision Trees**\n",
        "\n",
        "| Limitation                            | Explanation                                                                               |\n",
        "| ------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
        "| ‚ùå **Overfitting**                     | Especially with deep trees ‚Äî they may memorize the training data.                         |\n",
        "| ‚ùå **Instability**                     | Small changes in data can result in a very different tree structure.                      |\n",
        "| ‚ùå **Biased splits**                   | Toward features with more levels/categories.                                              |\n",
        "| ‚ùå **Not always optimal**              | Greedy splitting may not find the globally best tree.                                     |\n",
        "| ‚ùå **Poor with complex relationships** | May struggle with highly intricate patterns without ensemble methods like Random Forests. |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **When to Use Decision Trees**\n",
        "\n",
        "* When **interpretability** is crucial (e.g., in healthcare or legal decisions).\n",
        "* When you need a **quick, baseline model**.\n",
        "* For problems with **mixed data types** and limited preprocessing time.\n",
        "\n"
      ],
      "metadata": {
        "id": "sfZfIPban99i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances**"
      ],
      "metadata": {
        "id": "NVwAoUG-oO00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTxlkpAAoj-i",
        "outputId": "370bce09-84fc-4200-dcde-13c5460e264b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "‚óè Load the California Housing dataset from sklearn\n",
        "‚óè Train a Decision Tree Regressor\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances**"
      ],
      "metadata": {
        "id": "gndt-X4OosOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnjX0A9MowGN",
        "outputId": "3f9f138b-78cd-4a21-b553-fe0d55ff3f88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "‚óè Print the best parameters and the resulting model accuracy**"
      ],
      "metadata": {
        "id": "l6XZipKBo7OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Create the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_eteSmbpA7l",
        "outputId": "7c20a3d6-73e7-47d9-d021-f12165be3d4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.**"
      ],
      "metadata": {
        "id": "y2POLJ0zpL2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# For numerical features\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "X_num = num_imputer.fit_transform(X_num)\n",
        "\n",
        "# For categorical features\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_cat = cat_imputer.fit_transform(X_cat)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_imputer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [3, 5, 10],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "pN9gvf71p3DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}